{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2535,"status":"ok","timestamp":1700269675206,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"o9LGLwN6k7Lb","outputId":"20346c6f-aac7-4100-d7be-14b26b3fe341"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/narou\n"," 0_eval.txt\t\t\t     .MANGA.h5\n"," 1_eval.txt\t\t\t     MANGA.png\n"," all_metadata.csv\t\t     MANGA_SECONDARY_CHARACTERS.png\n"," .ANIME.h5\t\t\t     ncodes_0929\n"," ANIME.png\t\t\t     ncodes_0929.txt\n"," ANIME_SECONDARY_CHARACTERS.png      ncodes_1004\n"," array_reduce.ipynb\t\t     ncodes_1004.txt\n"," autoencoder_dim12.pth\t\t     ncodes_HighPoint.txt\n"," autoencoder.pth\t\t     ncodes_v1.txt\n"," bin_corpus_low\t\t\t     ncodes_v2.npy\n"," bin_corpus_patch\t\t     ncodes_v2.txt\n"," bin_corpus_v1\t\t\t     .OFFICIAL_ILLUSTRATIONS.h5\n"," bottom500.txt\t\t\t     OFFICIAL_ILLUSTRATIONS.png\n","'comformer_AEeval.ipynb '\t     OFFICIAL_ILLUSTRATIONS_SECONDARY_CHARACTERS.png\n"," comformer_AEeval_sample.ipynb\t     poolformer_for_P.pth\n"," comformer_AE.ipynb\t\t    'poolformer_only_body.ipynb '\n"," comformer_dim12.ipynb\t\t     poolformer_only_body_v2.ipynb\n"," comformer_dim48.ipynb\t\t     poolformer_only_head.ipynb\n"," corpus10_04\t\t\t     poolformer.pth\n"," corpus9_29\t\t\t     poolformer_v0.5.ipynb\n"," corpus_patch\t\t\t     poolformer_v0.ipynb\n"," corpus_v1\t\t\t     poolformer_v1.ipynb\n"," corpus_v2\t\t\t     poolformer_withlabel.pth\n"," craw_list\t\t\t     predict_logCB.csv\n"," .DANBOORU.h5\t\t\t    'predict_model_v1.ipynb '\n"," DANBOORU.png\t\t\t     predict_v1.csv\n"," DANBOORU_SECONDARY_CHARACTERS.png   pth_list.txt\n"," data_ae.csv\t\t\t     td_ae12.txt\n"," data_body.csv\t\t\t     td_ae.txt\n"," data_v0.5.csv\t\t\t     terminal.ipynb\n"," data_v1.5.csv\t\t\t     text_binary.ipynb\n"," data_v1.6.csv\t\t\t     top2000.txt\n"," data_v1.csv\t\t\t     train_loss_log\n"," data_v2.csv\t\t\t     train_loss_log_ae12.txt\n"," data_v3.csv\t\t\t     train_loss_log_ae.txt\n"," de_dim12_loss.csv\t\t     train_loss_log_p\n"," de_dim48_loss.csv\t\t     train_loss_log_v1.6\n"," dimension_reducer_v1.pth\t     train_loss_log_withlabel\n"," dimension_reducer_v2.pth\t     VAE.ipynb\n"," encoder_v0.5.pth\t\t     valid_data_ae.csv\n"," encoder_v1.5.pth\t\t     valid_data_body.csv\n"," encoder_v1.6.pth\t\t     valid_loss_log\n"," encoder_v1.pth\t\t\t     valid_loss_log_ae12.txt\n"," encoder_v2.pth\t\t\t     valid_loss_log_ae.txt\n"," ..GochiUsa_Dataset.pdf\t\t     valid_loss_log_p\n"," gochiusa_faces\t\t\t     valid_loss_log_v1.6\n"," gochiuza_faces.zip\t\t     valid_loss_log_withlabel\n"," head_v1.pth\t\t\t     vd_ae12.txt\n"," HighPoint.csv\t\t\t     vd_ae.txt\n"," log.txt\n","/content/drive/MyDrive/narou\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/narou\n","!ls -a\n","!pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16695,"status":"ok","timestamp":1700269691896,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"TzZGlbbJkopB","outputId":"45d45d20-aca5-4aec-8a37-db09ecfbd43a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n","Sat Nov 18 01:08:11 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!pip install tqdm\n","from psutil import virtual_memory\n","import torch\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb \u003c 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') \u003e= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1410,"status":"ok","timestamp":1700269693301,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"TP8D6bwdkz7F"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","from tqdm import tqdm\n","import math\n","import time"]},{"cell_type":"markdown","metadata":{"id":"Spx6PKCnnDrC"},"source":["dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700269693301,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"isEYFTrfm8g-"},"outputs":[],"source":["class FileDataset(torch.utils.data.Dataset):\n","    def __init__(self, corpus, files, max_length):\n","        self.corpus=corpus\n","        self.max_length=max_length\n","        self.files = files\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        torch.cuda.empty_cache()\n","        seq=torch.load(os.path.join(self.corpus, self.files[idx]))\n","        seq=seq[:self.max_length]\n","        time.sleep(0.5)\n","        return seq\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700269693301,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"GMxk0YS0VSXq"},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","def My_collate_func(batch):\n","    xs= []\n","    for x in batch:\n","        xs.append(x)\n","\n","\n","\n","    #データ長を揃える処理\n","    xs = pad_sequence(xs, batch_first=True)\n","\n","\n","    return xs"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700269693301,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"jYVxPOORG6v_"},"outputs":[],"source":["def manual_pad(x):\n","    batch_size, length, d = x.shape\n","    if length \u003c 128:\n","\n","        p=torch.zeros(batch_size, 128-length, d)\n","        x=torch.concat([x, p], dim = 1)\n","        print(f\"padding to size:{x.shape}\")\n","    else:\n","        pass\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"1oCj9hxcm18p"},"source":["poolformer"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700269693301,"user":{"displayName":"王劭宇","userId":"09863575444976027726"},"user_tz":-540},"id":"r12SmpOEmXcg"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, dim, dropout = 0.0, max_len = 512*144//4):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        #self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        position = torch.arange(max_len).unsqueeze(1).to(device)\n","        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim)).to(device)\n","        pe = torch.zeros(max_len, 1, dim).to(device)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        batch_size, max_len, dim = x.shape\n","        #cls_tokens = self.cls_token.repeat(batch_size, 1, 1)\n","        #x = torch.concat([cls_tokens, x], dim = 1)\n","        pe=torch.transpose(self.pe[:max_len], 0,1)\n","        x = x + pe.repeat(batch_size, 1,1)\n","        return self.dropout(x)\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, dim, head_num, dropout = 0.0):\n","        super().__init__()\n","        self.dim = dim\n","        self.head_num = head_num\n","        self.linear_Q = nn.Linear(dim, dim, bias = False)\n","        self.linear_K = nn.Linear(dim, dim, bias = False)\n","        self.linear_V = nn.Linear(dim, dim, bias = False)\n","        self.linear = nn.Linear(dim, dim, bias = False)\n","        self.soft = nn.Softmax(dim = 3)\n","        self.dropout = nn.Dropout(dropout)\n","    def split_head(self, x):\n","        x = torch.tensor_split(x, self.head_num, dim = 2)\n","        x = torch.stack(x, dim = 1)\n","        return x\n","    def concat_head(self, x):\n","        x = torch.tensor_split(x, x.size()[1], dim = 1)\n","        x = torch.concat(x, dim = 3).squeeze(dim = 1)\n","        return x\n","\n","    def forward(self, Q, K, V, mask = None):\n","        Q = self.linear_Q(Q)   #(BATCH_SIZE,word_count,dim)\n","        K = self.linear_K(K)\n","        V = self.linear_V(V)\n","\n","        Q = self.split_head(Q)   #(BATCH_SIZE,head_num,word_count//head_num,dim)\n","        K = self.split_head(K)\n","        V = self.split_head(V)\n","\n","        QK = torch.matmul(Q, torch.transpose(K, 3, 2))\n","        QK = QK/((self.dim//self.head_num)**0.5)\n","\n","        if mask is not None:\n","        #print(f\"QK:{np.shape(QK)}, mask:{np.shape(mask)}\")\n","            QK = QK + mask\n","\n","        softmax_QK = self.soft(QK)\n","        softmax_QK = self.dropout(softmax_QK)\n","\n","        QKV = torch.matmul(softmax_QK, V)\n","        QKV = self.concat_head(QKV)\n","        QKV = self.linear(QKV)\n","        return QKV\n","\n","class FeedForward(nn.Module):\n","\n","    def __init__(self, dim, dropout = 0.0):\n","        super().__init__()\n","        hidden_dim=int(dim*2)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_1 = nn.Linear(dim, hidden_dim)\n","        self.gelu = nn.GELU()\n","        self.linear_2 = nn.Linear(hidden_dim, dim)\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.gelu(x)\n","        x = self.dropout(x)\n","        x = self.linear_2(x)\n","        return x\n","\n","class transblock(nn.Module):\n","\n","    def __init__(self, dim, head_num, dropout = 0.0):\n","        super().__init__()\n","        self.MHA = MultiHeadAttention(dim, head_num)\n","        self.layer_norm_1 = nn.LayerNorm([dim])\n","        self.layer_norm_2 = nn.LayerNorm([dim])\n","        self.FF = FeedForward(dim)\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        Q = K = V = x\n","        x = self.layer_norm_1(x)\n","        x = self.MHA(Q, K, V)\n","        x = self.dropout_1(x)\n","        x = x + Q\n","        _x = x.clone()\n","        x = self.layer_norm_2(x)\n","        x = self.FF(x)\n","        x = self.dropout_2(x)\n","        x = x + _x\n","        return x\n","\n","\n","class transformer(nn.Module):\n","    def __init__(self, channel, kernal_size=2, layer=4, dropout = 0.0, reverse=False):\n","        super().__init__()\n","\n","        self.conv=ConvProj(channel, channel, kernal_size, stride=kernal_size, reverse=reverse)\n","        self.pe=PositionalEncoding(channel)\n","        self.transformer = nn.Sequential(*[transblock(channel, channel//4) for _ in range(layer)])\n","\n","    def forward(self, x):\n","        x=self.conv(x)\n","        x = torch.transpose(x, 1, 2)\n","        x=self.pe(x)\n","        x=self.transformer(x)\n","        x = torch.transpose(x, 1, 2)\n","        return x\n","\n","class ConvProj(nn.Module):\n","    def __init__(self, in_channel, out_channel, kernal_size=2, stride=2, reverse=False):\n","        super().__init__()\n","\n","\n","        self.point=nn.Conv1d(in_channel, out_channel, 1, 1, padding=0)\n","\n","        if reverse==False:\n","            self.conv2=nn.Conv1d(out_channel, out_channel, kernal_size, stride, groups=out_channel, padding=0)\n","            self.pool=nn.AvgPool1d(kernal_size, stride=kernal_size, padding=0)\n","        else:\n","            self.conv2=nn.ConvTranspose1d(out_channel, out_channel, kernal_size, stride, groups=out_channel, padding=0)\n","            self.pool=nn.ConvTranspose1d(out_channel, out_channel, kernal_size, stride, groups=out_channel, padding=0)\n","        self.conv1=nn.Conv1d(in_channel,out_channel, 1, 1, padding=0)\n","        self.conv3=nn.Conv1d(out_channel, out_channel, 1, 1,  padding=0)\n","        self.gelu=nn.GELU()\n","        self.norm1=nn.GroupNorm(1, out_channel)\n","        self.norm2=nn.GroupNorm(1, out_channel)\n","    def shortcut(self, x):\n","        x=self.point(x)\n","        x=self.pool(x)\n","        return x\n","\n","\n","\n","    def forward(self, x):\n","        _x=self.shortcut(x)\n","        x=self.conv1(x)\n","        x=self.norm1(x)\n","        x=self.gelu(x)\n","        x=self.conv2(x)\n","        x=self.conv3(x)\n","        x=self.norm2(x)\n","        x=self.gelu(x)\n","        x+=_x\n","\n","        return x\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, channel, kernal_size=3, stride=1):\n","        super().__init__()\n","\n","        self.shortcut=nn.Sequential(nn.Identity())\n","\n","        self.conv2=nn.Conv1d(channel//2, channel//2, kernal_size, stride, groups=channel//2, padding=kernal_size//2)\n","        self.conv1=nn.Conv1d(channel,channel//2, 1, 1, padding=0)\n","        self.conv3=nn.Conv1d(channel//2, channel, 1, 1,  padding=0)\n","        self.gelu=nn.GELU()\n","        self.norm1=nn.GroupNorm(1, channel//2)\n","        self.norm2=nn.GroupNorm(1, channel)\n","\n","\n","    def forward(self, x):\n","        _x=self.shortcut(x)\n","        x=self.conv1(x)\n","        x=self.norm1(x)\n","        x=self.gelu(x)\n","        x=self.conv2(x)\n","        x=self.conv3(x)\n","        x=self.norm2(x)\n","        x=self.gelu(x)\n","        x+=_x\n","        return x\n","\n","class stage(nn.Module):\n","    def __init__(self, channel, kernal_size=2, stride=2, min_ch=48, reverse=False):\n","        super().__init__()\n","        if reverse == False:\n","            out_channel=channel//2\n","        else:\n","            out_channel=channel*2\n","\n","        self.conv1=ConvProj(channel, out_channel, kernal_size, stride=stride, reverse=reverse)\n","        self.conv2=nn.Sequential(*[ConvBlock(out_channel,kernal_size=3) for _ in range(3)])\n","\n","        if out_channel \u003c= min_ch:\n","            self.next_stage=nn.Identity()\n","        else:\n","            if  out_channel \u003e= 768 :\n","                self.next_stage=nn.Identity()\n","            else:\n","                self.next_stage=stage(out_channel, kernal_size=2, stride=2, reverse=reverse)\n","\n","    def forward(self, x):\n","        #print(x.shape)\n","        x=self.conv1(x)\n","        x=self.conv2(x)\n","        x=self.next_stage(x)\n","\n","        return x\n","\n","\n","\n","class autoencoder(nn.Module):\n","    def __init__(self, min_ch=48):\n","        super().__init__()\n","        self.down_stage=stage(768, min_ch=min_ch)\n","        self.up_stage=stage(48, min_ch=min_ch, reverse=True)\n","        self.trans1=transformer(48)\n","        self.trans2=transformer(48)\n","        self.trans3=transformer(48, reverse=True)\n","        self.trans4=transformer(48, reverse=True)\n","        self.linear=nn.Linear(768, 768)\n","\n","    def forward(self, x):\n","        x = torch.transpose(x, 1, 2)\n","        x0=x.shape\n","        x=self.down_stage(x)\n","        x=self.trans1(x)\n","        x=self.trans2(x)\n","        x=self.trans3(x)\n","        x=self.trans4(x)\n","        x=self.up_stage(x)\n","        #print(x0, x.shape)\n","        x = torch.transpose(x, 1, 2)\n","        x=self.linear(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Ywf6Zi-9mlTU"},"source":["train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1KNPzgJP2D_omMfGRTmr8xfcskH9Jb-xD"},"id":"wc55fweumkBM","outputId":"bbbdc2c9-4174-461c-8add-b1d00813b76a"},"outputs":[],"source":["\n","def train(device):\n","    max_length = 128*256\n","    batch_size = 1\n","    epochs = 32\n","    lr =  3e-5\n","    valid_rate = 0.05\n","    min_ch=48\n","    model = torch.load(\"autoencoder.pth\").to(device)\n","    #model=autoencoder(min_ch=48).to(device)\n","    criterion = nn.MSELoss().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n","    corpus = \"bin_corpus_patch\"\n","\n","\n","    try:\n","        with open(\"td_ae.txt\", \"r\") as t:\n","            td=t.read().split()\n","        with open(\"vd_ae.txt\", \"r\") as v:\n","            vd=v.read().split()\n","    except:\n","        files=os.listdir(corpus)\n","        with open(\"top2000.txt\", \"r\") as ls:\n","            ncodes=ls.read().split()\n","        files=[f for f in files if f[:-4] in ncodes]\n","        td=files[int(len(files)*valid_rate):]\n","        vd=files[:int(len(files)*valid_rate)]\n","        with open(\"td_ae.txt\", \"w\") as t:\n","            t.write(\"\\n\".join(td))\n","        with open(\"vd_ae.txt\", \"w\") as v:\n","            v.write(\"\\n\".join(vd))\n","\n","    train_dataset = FileDataset(corpus, td, max_length)\n","    valid_dataset = FileDataset(corpus, vd, max_length)\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset,  batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=My_collate_func, num_workers=0)\n","    val_dataloader = torch.utils.data.DataLoader(valid_dataset,  batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=My_collate_func, num_workers=0)\n","\n","\n","\n","    try:\n","        with open(\"train_loss_log_ae.txt\", \"r\") as tl:\n","            train_loss_list=tl.read().split()\n","\n","        train_loss_list=[float(n) for n in train_loss_list]\n","        with open(\"valid_loss_log_ae.txt\", \"r\") as vl:\n","            valid_loss_list=vl.read().split()\n","        valid_loss_list=[float(n) for n in valid_loss_list]\n","    except:\n","        train_loss_list=[]\n","        valid_loss_list=[]\n","    print(train_loss_list)\n","    print(valid_loss_list)\n","    for epoch in range(len(train_loss_list)+1,epochs+1):\n","\n","        model.train()\n","        total_loss_train=0.0\n","        n=0\n","        batsu=0\n","\n","        for seq in tqdm(train_dataloader):\n","\n","            #print(f\"seq length:{seq.shape[1]//128}\")\n","            if seq.shape[1] \u003c 128:\n","                #seq=manual_pad(seq)\n","                continue\n","            #elif n \u003e 6:\n","            #    break\n","            else:\n","                pass\n","            seq=seq.to(device)\n","            #print(seq.shape)\n","            optimizer.zero_grad()\n","            output=model(seq)\n","            N, Li, D = seq.shape\n","            N, Lo, D = output.shape\n","            if Lo \u003e Li:\n","                output=output[:,:Li,:]\n","            else:\n","                seq=seq[:,:Lo,:]\n","\n","            loss=criterion(output, seq)\n","            loss.backward()\n","            total_loss_train+=loss.item()\n","            n += 1\n","            #print(f'running loss:{loss.item():5.7}')\n","            optimizer.step()\n","            del seq, output, loss\n","            torch.cuda.empty_cache()\n","            #!nvidia-smi --query-gpu=utilization.gpu --format=csv\n","            if n%1024==1:\n","                torch.save(model, \"autoencoder.pth\")\n","                #print(\"saved\")\n","        torch.save(model, \"autoencoder.pth\")\n","\n","\n","            #except Exception as e:\n","                #print(e)\n","\n","        total_loss_train=total_loss_train/len(train_dataloader)\n","        scheduler.step()\n","\n","        #valid\n","        model.eval()\n","        total_loss_valid=0.0\n","        with torch.no_grad():\n","            for seq in tqdm(val_dataloader):\n","                #try:\n","                if seq.shape[1] \u003c 128:\n","                    #seq=manual_pad(seq)\n","                    continue\n","                #elif n \u003e 10:\n","                #    break\n","                #else:\n","                #    pass\n","                    #optimizer.zero_grad()\n","                seq=seq.to(device)\n","                output=model(seq)\n","                N, Li, D = seq.shape\n","                N, Lo, D = output.shape\n","                if Lo \u003e Li:\n","                    output=output[:,:Li,:]\n","                else:\n","                    seq=seq[:,:Lo,:]\n","                loss_valid=criterion(output, seq)\n","                total_loss_valid+=loss_valid.item()\n","                del seq, output, loss_valid\n","                #print(f'for patch No.{n}, running loss:{loss_valid.item():5.7}')\n","                torch.cuda.empty_cache()\n","                n += 1\n","                #except Exception as e:\n","                    #print(e)\n","        total_loss_valid=total_loss_valid/len(val_dataloader)\n","\n","\n","\n","        train_loss_list.append(total_loss_train)\n","        valid_loss_list.append(total_loss_valid)\n","        with open(\"train_loss_log_ae.txt\", \"w\") as tl:\n","            tl.write('\\n'.join([str(n) for n in train_loss_list]))\n","        with open(\"valid_loss_log_ae.txt\", \"w\") as vl:\n","            vl.write('\\n'.join([str(n) for n in valid_loss_list]))\n","        print(f'{epoch:3d}:epoch | {total_loss_train:5.7} : train loss | {total_loss_valid:5.7} : valid loss')\n","\n","        if total_loss_train \u003c total_loss_valid:\n","            batsu+=1\n","        else:\n","            pass\n","        if  batsu \u003e 3:\n","            print(\"early stop\")\n","            #break\n","\n","\n","    plt.xlabel('epoch')\n","    plt.ylabel('train_loss')\n","    plt.plot(train_loss_list, color='r', label=\"train\")\n","    plt.plot(valid_loss_list, color='b', label=\"valid\")\n","    plt.show()\n","\n","\n","\n","    return model\n","\n","if __name__ == \"__main__\":\n","    train(device)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM0uZNEY+iQMlUpywdhG4kM","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}